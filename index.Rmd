---
title: "PML Course project"
author: "Gerrit Versteeg"
date: "20 July 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Executive summary
This research makes an effort to predict how well a weight lifting exercise is performed, based on a number of movement measures. After cleaning the training set from unnecessary variables (NA's in testing set and non-measure variables), a random forest model was fitted using 25 trees and 10-fold cross validation. This resulted in a model with a 99.1% accuracy on the training set and an out-of-bag error of 0.84%. The original markdown file and accompanying files are available [here](https://github.com/GVersteeg/PML-Project).

## 2. Loading and preparing the data
```{r dataloading, echo=TRUE, message=FALSE, warning=FALSE}
library("dplyr", quietly=TRUE, warn.conflicts = FALSE)     ## for data prep
library("caret", quietly=TRUE, warn.conflicts = FALSE)     ## also loads ggplot2
urlTR <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTS <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (file.exists("./plm-training.csv")) {unlink("./plm-training.csv")}
if (file.exists("./plm-testing.csv")) {unlink("./plm-testing.csv")}
download.file(urlTR, destfile="./pml-training.csv", method="curl")
download.file(urlTS, destfile="./pml-testing.csv", method="curl")
DF_Tr <- tbl_df(read.csv("pml-training.csv"))              ## load into dataset/tibble
DF_Ts <- tbl_df(read.csv("pml-testing.csv"))               ## load into dataset/tibble
```

## 2. Data-preparation and Feature Selection
The original [research paper](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) mentions the selection of 17 features (out of 96 measures) as being most relevant, but these features cannot be directly related to the 160 variables without in-depth knowledge of the measures. We will decrease the number of variables in two steps: 1) de-select all non-measurement variables (columns 1 to 7). 2) de-select all variables that have NA's in the testing set, because these have no value during the evaluation of the model to be build.
```{r dataprep, ECHO = TRUE}
## create vector of column-ids for columns having NA's or no measures
v1 <- c(1:7)                            ## init vector (first 7 column-indices)
for(i in 8:ncol(DF_Ts)){                ## for all other columns
        if (sum(is.na(DF_Ts[,i]))>0) {  ## if any NA exists in column
                v1<-c(v1,i)             ## add the column index to vector
        }                               ## v1 contains ref's to all unnecc.columns
}
dfTs <- DF_Ts[,-v1]                     ## keeping remaining 53 variables in testset
dfTr <- DF_Tr[,-v1]                     ## keeping the same 53 variables in trainset
dim(dfTr)
```

Training a classification problem can be impaired by an unbalanced distribution of the number of samples over the outcome classes. 
```{r imbalance, ECHO = TRUE}
## to look at possible imbalance of the no of samples for each outcome class:
table(DF_Tr$classe)
```
Although class "A' is favored, the other classes are nicely distributed, so the training set does not look very unbalanced as a whole.

## 3. Choosing the model
The outcome to be predicted is "classe", a factor variable (5 levels), hence this is a classification problem. The original [study](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) used 10 combined random forests each with 10 trees and 10-fold CV to avoid overfitting the model to the training set. Random forest tends to yield accurate results (next to boosting) in classification problems as stated in the lectures, so we are going to use a similar model as the one used by the original research.

A slight adaptation is made to avoid generating 10 random forests and then combining them. The research suggests that we should be using 10 forests * 10 trees (i.e. 100 trees in one forest), but we will start with 25 trees in one forest to avoid lengthy computing times and check whether this will generate sufficient accuracy. Also usage was made of the excellent tips from the forum blog of Len Gerski to speed up the training process (thanks Len).

```{r model, ECHO = TRUE, message=FALSE, warning=FALSE}
library(parallel)                               ## configure parallel processing
library(doParallel)                             ##
cluster <- makeCluster(detectCores()-1)         ## leave one core for the OS
registerDoParallel(cluster)                     ##
fC <- trainControl(method="cv",number=10,       ## set trainControl object
                   allowParallel=TRUE)          ## 
set.seed(46218)                                 ## set seed for reproducible results
fit <- train(classe~.,method="rf",data=dfTr,    ## train the rf model
             ntree=25,trcontrol=fC)             ##
stopCluster(cluster)                            ## release parallel processing cluster
registerDoSEQ()                                 ## 
print(fit)                                      ## print the model
```

## 4. Evaluating the model
It has no use to split up the training set into a separate training and validation set (to subsequently test our model), because we already used 10-fold cross-validation during the training of the model. The training already took care of our validation. Therefor we can you use the accuracy and out-of-sample errors that were automatically generated during the training.

With an accuracy of 0.9911093 (as shown above), mtry 27 was used for the final model. To check this accuracy we can use confusionMatrix set to a mode that takes a re-sample out of the training set.

```{r evaluate1, ECHO = TRUE}
confusionMatrix.train(fit)      ## .train because we don't have testset outcomes
```
The resulting average accuracy is **99.11%**. The same as the results during training.

The out-of-sample error (i.e out-of-bag error) is also relevant. It is automatically generated during training as well and listed in the final model.

```{r evaluate2, ECHO = TRUE}
fit$finalModel
```
The out-of-bag error is approx. **0.84%**, which is also low enough for our prediction.

Because we used less than 100 trees, we need to check whether we used enough trees. To do so we plot the final model and look at the reduction of misclassifications while increasing the number of trees used in the model.

```{r evaluate3, ECHO = TRUE}
plot(fit$finalModel)   ## to look at the error versus the number of trees
```


This shows that, while going from 20 to 25 trees, not much additional error-reduction is achieved, telling us that using 25 trees was sufficient to reach the needed accuracy.

## Predicting on the test set
Having an accurate model we can now make our prediction using the test set "dfTs" that we set aside in the beginning.

```{r predict, ECHO = TRUE}
pred <- predict(fit, dfTs)
pred
```

These predictions function as input for the course project quiz.
So we put them in a handy format:
```{r writeFile, ECHO = TRUE}
answers <- cbind(dfTs$problem_id, as.character(pred))
if (file.exists("./answersQuiz.csv")) {unlink("./answersQuiz.csv")}
write.csv(answers, file = "./answersQuiz.csv",row.names=FALSE)
```
